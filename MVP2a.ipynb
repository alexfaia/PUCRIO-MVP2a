{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexfaia/PUCRIO-MVP2a/blob/main/MVP2a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data Preperation and Exploration"
      ],
      "metadata": {
        "id": "0S8UkBSxlsNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages and libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import shapiro\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZTJP9AO4lsNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file\n",
        "df = pd.read_csv(r\"C:\\Users\\Alex Faia\\Downloads\\puc\\ObesityDataSet.csv\")"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ihby9eXlsNZ",
        "outputId": "6e0d0ad5-f7cb-44fb-d2f2-dc30af512459"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5a39dda9e780>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\Alex Faia\\Downloads\\puc\\ObesityDataSet.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Alex Faia\\\\Downloads\\\\puc\\\\ObesityDataSet.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About the Data\n",
        "\n",
        "This data comes from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+). This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition.\n",
        "\n",
        "### Notes:\n",
        "<ul>\n",
        "    <li> This data is created via survery. However the data was imbalanced and records were synthasized using WEKA to simulate additional responses.\n",
        "    <li> The synthasized data fits the distribution of the data but generates floats (ratio data) where ordinal and integer data were the original values in the survey.\n",
        "    <li> The target variable is comprised of the variables height and weight. Height and weight are used to classisfy a persons BMI. The BMI measure was then used to classify each observation.\n",
        "</ul>\n",
        "\n",
        "### Features & Descriptions\n",
        "\n",
        "<table>\n",
        "    <thead>\n",
        "        <tr><th>Category</th><th>Feature Name</th><th>Description</th><th>Variable Type</th></tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr><td>Target Variable</td><td>NObesity</td><td>Based on BMI</td><td>Categorical</td></tr>\n",
        "        <tr><td>Eating Habits</td><td>FAVC</td><td>Frequent consumption of high caloric food</td><td>Categorical</td></tr>\n",
        "        <tr><td>Eating Habits</td><td>FCVC</td><td>Frequency of consumption of vegetables</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Eating Habits</td><td>NCP</td><td>Number of main meals</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Eating Habits</td><td>CAEC</td><td>Consumption of food between meals</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Eating Habits</td><td>CH20</td><td>Consumption of water daily</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Eating Habits</td><td>CALC</td><td>Consumption of alcohol</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Physical Conditioning</td><td>SCC</td><td>Calories consumption monitoring</td><td>Categorical</td></tr>\n",
        "        <tr><td>Physical Conditioning</td><td>FAF</td><td>Pysical activity frequency</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Physical Conditioning</td><td>TUE</td><td>Time using technology devices</td><td>Ordinal</td></tr>\n",
        "        <tr><td>Physical Conditioning</td><td>MTRANS</td><td>Transportation used</td><td>Categorical</td></tr>\n",
        "        <tr><td>Physical Conditioning</td><td>SMOKE</td><td>Smokes Yes or No</td><td>Categorical</td></tr>\n",
        "        <tr><td>Responder Charateristics</td><td>Family History with Overweight</td><td>Yes or No</td><td>Categorical</td></tr>\n",
        "        <tr><td>Responder Charateristics</td><td>Gender</td><td>Gender is Male or Female</td><td>Categorical</td></tr>\n",
        "        <tr><td>Responder Charateristics</td><td>Age</td><td>Age in years</td><td>Integer</td></tr>\n",
        "        <tr><td>Responder Charateristics</td><td>Height</td><td>Height in meters</td><td>Float</td></tr>\n",
        "        <tr><td>Responder Charateristics</td><td>Weight</td><td>Weight in kilograms</td><td>Float</td></tr>        \n",
        "     </tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Qvi-KYzclsNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Exploring the Data & Clean Up\n",
        "\n",
        "- The data contains 2111 records with 17 columns\n",
        "- The data loads as text and float objects for most of the objects. However we know that some are float, categorical and ordinal\n",
        "- All of the records are unique and contain no null values\n",
        "- Height and Weight are included however they have a direct correlation to each other and our target variable\n",
        "- The survey data is distinguishable from the synthazied data based on floats used for ordinal variables\n"
      ],
      "metadata": {
        "id": "rRQfGRHFlsNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 rows show survey data\n",
        "df.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wPvoUpjQlsNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bottom 5 rows show synthetic data\n",
        "df.tail(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "XcU6T20jlsNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional rows showing synthetic data\n",
        "df.iloc[[501,518,516]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "V9F_6HrdlsNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Prep / Clean Up\n",
        "\n",
        "- Drop Height and Weight columns as they are used in the BMI calculation for our target variable\n",
        "- Convert categorical variables to category instead of object/text\n",
        "- Convert synthetic floats & floats to whole integers to better reprsent the ordinal data from orignal survey data"
      ],
      "metadata": {
        "id": "yWS630iIlsNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Height and weight are highly correlated and they directly correlate to the BMI calc used for the target\n",
        "# Remove Height and Weight\n",
        "df = df.drop(columns=['Height', 'Weight'])\n",
        "print(df.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZB4hDlpHlsNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no nulls\n",
        "df[df.isnull().any(axis=1)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "LJaiUxN6lsNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert object/text variables to category variables\n",
        "columns = [\"Gender\", \"family_history_with_overweight\", \"FAVC\", \"CAEC\", \"SMOKE\", \"SCC\", \"CALC\", \"MTRANS\", \"NObeyesdad\"]\n",
        "\n",
        "for col in columns:\n",
        "    df[col] = df[col].astype('category')"
      ],
      "metadata": {
        "trusted": true,
        "id": "1-dkyIzIlsNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to interigate data after conversion\n",
        "# provides min, max, unique counts\n",
        "def variable_counts(columns, stage):\n",
        "\n",
        "    if stage == 'pre':\n",
        "        print(\"Pre Conversion to Integer\")\n",
        "    else:\n",
        "        print(\"Post Conversion to Integer\")\n",
        "\n",
        "    for col in columns:\n",
        "        print(\"Variable:\", col, \"| Count Unique:\",df[col].nunique(),\"| Min: \", df[col].min(), \"| Max: \",df[col].max())"
      ],
      "metadata": {
        "trusted": true,
        "id": "iEixRM7ElsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert float variables to integer to the nearest inter\n",
        "columns = [\"FCVC\", \"NCP\", \"CH2O\", \"TUE\", \"FAF\"]\n",
        "\n",
        "# pre conversion countss\n",
        "variable_counts(columns, 'pre')\n",
        "\n",
        "# convert to int / nearest int value\n",
        "for col in columns:\n",
        "    #round to nearest whole number\n",
        "    df[col] = round(df[col]).astype('int')\n",
        "\n",
        "# post conversion counts\n",
        "print(\"\")\n",
        "variable_counts(columns, 'post')"
      ],
      "metadata": {
        "trusted": true,
        "id": "TZLjCanqlsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm types\n",
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kogzqe8ylsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# review non synthetic are still the same\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "pMPJruZBlsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Intuition & Further Exploration\n",
        "\n",
        "##### Categorical Variables\n",
        "- The categorical variables are not gaussian\n",
        "- Most of the categorical variables are bernoulli in nature\n",
        "- The target variables based on synthetic process are fairly balanced\n",
        "\n",
        "##### Ordinal Variables                                        \n",
        "- Will be treated as nominal         \n",
        "- Non are Gaussian distributed\n",
        "\n",
        "##### Ratio Variable\n",
        "- Age is the only ratio varible\n",
        "- Is not Gaussian\n",
        "\n",
        "##### Predictor Coorelations\n",
        "- The predictor variables not highly correlated\n",
        "- Height and Weight have been removed for their correlation to each other and the target\n",
        "\n",
        "##### Target Variable\n",
        "- Is categorical with > 2 classes\n",
        "- Is faily balanced in its distribution of weights"
      ],
      "metadata": {
        "id": "wSAxniWJlsNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# columns of interest\n",
        "columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE',\n",
        "           'SCC', 'CALC', 'MTRANS', 'NObeyesdad']\n",
        "\n",
        "fig, ax = plt.subplots(3, 3, figsize=(15, 10))\n",
        "for col, subplot in zip(columns, ax.flatten()):\n",
        "    sns.countplot(df[col], ax=subplot)\n",
        "\n",
        "    if col==\"MTRANS\":\n",
        "        sns.countplot(df[col],ax=subplot)\n",
        "        subplot.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.MTRANS)\n",
        "        subplot.yaxis.label.set_text(\"Number of Records\")\n",
        "    elif col==\"NObeyesdad\":\n",
        "        sns.countplot(df[col],ax=subplot)\n",
        "        subplot.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.NObeyesdad)\n",
        "        subplot.yaxis.label.set_text(\"Number of Records\")\n",
        "    else:\n",
        "        sns.countplot(df[col],ax=subplot)\n",
        "        subplot.yaxis.label.set_text(\"Number of Records\")\n",
        "\n",
        "# show figure & plots\n",
        "fig.suptitle(\"Categorigal Variables\", fontsize=20)\n",
        "plt.tight_layout(pad=5, w_pad=0.0, h_pad=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "UCwZkY9GlsNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns of interest\n",
        "columns = [\"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(15, 4))\n",
        "for col, subplot in zip(columns, ax.flatten()):\n",
        "    sns.countplot(df[col], ax=subplot)\n",
        "    subplot.yaxis.label.set_text(\"Number of Records\")\n",
        "\n",
        "# show figure & plots\n",
        "fig.suptitle(\"Ordinal Variables\", fontsize=20)\n",
        "plt.tight_layout(pad=5, w_pad=0.7, h_pad=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "0Oo-1CW1lsNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ratio variable distribution\n",
        "\n",
        "fig = plt.figure(figsize = (16,5))\n",
        "\n",
        "#distplot\n",
        "ax1 = fig.add_subplot(121)\n",
        "sns.distplot(df[\"Age\"], kde=True)\n",
        "\n",
        "#boxplot\n",
        "ax1 = ax1 = fig.add_subplot(122)\n",
        "sns.boxplot(df.Age)\n",
        "\n",
        "# show figure & plots\n",
        "fig.suptitle(\"Distribution of Numeric (Ratio) Variable\", fontsize=20)\n",
        "plt.tight_layout(pad=5, w_pad=0.5, h_pad=.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "TkAXrZNxlsNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create figure\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# add subplot for one row 2 graphs first postion\n",
        "ax1 = fig.add_subplot(121)\n",
        "\n",
        "# correlation data matrix\n",
        "matrix = np.triu(df.corr())\n",
        "\n",
        "# set title\n",
        "ax1.title.set_text(\"Coorelation Heatmap: Predictor Variables\")\n",
        "\n",
        "#define plot\n",
        "sns.heatmap(df.corr(),\n",
        "                 mask=matrix,\n",
        "                 annot = False,\n",
        "                 fmt='.1g',\n",
        "                 cmap=\"YlGnBu\",\n",
        "                 vmin=-1, vmax=1, center= 0,\n",
        "                 square=\"True\",\n",
        "                 ax=ax1)\n",
        "\n",
        "# add second subplot\n",
        "ax2 = fig.add_subplot(122)\n",
        "\n",
        "# rotate axis label\n",
        "ax2.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.NObeyesdad)\n",
        "\n",
        "# Set title text\n",
        "ax2.title.set_text(\"Weight Category Counts: Target Variable\")\n",
        "\n",
        "# define second plot\n",
        "sns.countplot(x=\"NObeyesdad\",\n",
        "                 palette=\"Blues_d\",\n",
        "                 order=df.NObeyesdad.value_counts().index,\n",
        "                 ax = ax2,\n",
        "                 data=df)\n",
        "\n",
        "# labels for x and y\n",
        "ax2.xaxis.label.set_text(\"Level Category\")\n",
        "ax2.yaxis.label.set_text(\"Number of Records\")\n",
        "\n",
        "# turn off top and right frame lines\n",
        "ax2.spines['right'].set_visible(False)\n",
        "ax2.spines['top'].set_visible(False)\n",
        "\n",
        "# show figure & plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "yITduY7_lsNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create correlation matrix\n",
        "corr_matrix = df.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "\n",
        "#print highly correlated variables\n",
        "print(\"Number of variables with > 0.95 correlation: \", len(to_drop))"
      ],
      "metadata": {
        "trusted": true,
        "id": "JIEsPIeElsNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Intuition & Model Considerations\n",
        "\n",
        "##### Data Intuion\n",
        "- Models for this data need to cater to > 2 two classes\n",
        "- Data is limited so model will need to perform well with limited amount of data\n",
        "- Model will need to handle dummy variables well and not be dependent on ratio data\n",
        "- The ordinal variables may or may not be helful to the model\n",
        "- The data is not Gaussian so the model needs to be non-parametic or at least not strictly parametic\n",
        "- The data is pretty wide but not very deep, so reducing the factors may be necessary\n",
        "\n",
        "##### Potential Models\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Search Vector Machines (SVM)\n",
        "\n",
        "##### Experiments\n",
        "Models that may be used to evaluate data assumptions and model performance:\n",
        "- Is the data too limited for a Nerual Network?\n",
        "- Does Logistic Regression perform well if target variable reduced to 2 classes?\n",
        "- Do the other models perform better if target variable is reduced to 2 classes?"
      ],
      "metadata": {
        "id": "Xsendde_lsNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Machine Learning\n",
        "\n",
        "The target variable is NObesity. The attempt is is apply ML to find the best model for predicting NObesity.\n",
        "NObesity is a categorical variable that is a measure of a person weight ranging from under weight to very obese (Overweight Level II)."
      ],
      "metadata": {
        "id": "gTGemoIWlsNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data prep for ML models: General\n",
        "- Some models may require additional treatment\n",
        "- These steps will prep the data for the most viable models: Decision Trees, Random Forest, SVM, Nerual Network\n",
        "- Some models may benifit from scaling so this will be evaluated as well\n",
        "\n",
        "#### Data Treatment\n",
        "- Copy cleaned data to new dataframe\n",
        "- Create dummy variables out of categorical variables\n",
        "- Split the data into 70/30 train & test datasets     "
      ],
      "metadata": {
        "id": "SH8TNRAXlsNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep = df.copy()"
      ],
      "metadata": {
        "trusted": true,
        "id": "QHX9P9H4lsNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dummy variables\n",
        "df_prep = pd.get_dummies(df_prep,columns=[\"Gender\",\"family_history_with_overweight\",\n",
        "                                          \"FAVC\",\"CAEC\",\"SMOKE\",\"SCC\",\"CALC\",\"MTRANS\"])\n",
        "df_prep.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "XLDOOtB0lsNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset in features and target variable\n",
        "\n",
        "# Features\n",
        "X = df_prep.drop(columns=[\"NObeyesdad\"])\n",
        "\n",
        "# Target variable\n",
        "y = df_prep['NObeyesdad']"
      ],
      "metadata": {
        "trusted": true,
        "id": "RzT76ZIalsNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn packages for data treatments\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
      ],
      "metadata": {
        "trusted": true,
        "id": "Djw453x5lsNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Models\n",
        "\n",
        "For this exercise we will take a look at\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Support Vector Machines (SVM)\n",
        "- K Nearest Neighbors\n",
        "\n",
        "Cursory Look at the model results suggest that Random Forest will be our best initial model with an accuracy of 79%. However, many of the other models performance is not too far off.\n",
        "\n",
        "The other interesting result is that all of the models classify Obesity_Type_III amazingly well with >= 98% accuracy across all models."
      ],
      "metadata": {
        "id": "6fmjDGdolsNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn.preprocessing import StandardScaler # Import for standard scaling of the data\n",
        "from sklearn.preprocessing import MinMaxScaler # Import for standard scaling of the data\n",
        "\n",
        "# standard scale data\n",
        "ss = StandardScaler()\n",
        "X_train_scaled = ss.fit_transform(X_train)\n",
        "X_test_scaled = ss.transform(X_test)\n",
        "\n",
        "# tested MinMaxScaler as KNN historically does better with MinMax\n",
        "mm = MinMaxScaler()\n",
        "X_train_mm_scaled = ss.fit_transform(X_train)\n",
        "X_test_mm_scaled = ss.transform(X_test)\n",
        "\n",
        "# program to run multilple models though sklearn\n",
        "# Default settings output accuracy and classification report\n",
        "# compares accuracy for scaled and unscaled data\n",
        "def run_models(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
        "\n",
        "    models = [\n",
        "          ('Random Forest', RandomForestClassifier(random_state=2020)),\n",
        "          ('Decision Tree', DecisionTreeClassifier()),\n",
        "          ('KNN', KNeighborsClassifier()),\n",
        "          ('SVM', SVC())\n",
        "        ]\n",
        "\n",
        "    for name, model in models:\n",
        "        # unscaled data\n",
        "        clf = model.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        # scaled data\n",
        "        clf_scaled = model.fit(X_train_scaled, y_train)\n",
        "        y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "        # mm scaled data\n",
        "        clf_mm_scaled = model.fit(X_train_mm_scaled, y_train)\n",
        "        y_pred_mm_scaled = clf_scaled.predict(X_test_mm_scaled)\n",
        "\n",
        "        # accuracy scores\n",
        "        accuracy = round(metrics.accuracy_score(y_test, y_pred),5)\n",
        "        scaled_accuracy = round(metrics.accuracy_score(y_test, y_pred_scaled),5)\n",
        "        scaled_mm_accuracy = round(metrics.accuracy_score(y_test, y_pred_mm_scaled),5)\n",
        "\n",
        "        # output\n",
        "        print(name + ':')\n",
        "        print(\"---------------------------------------------------------------\")\n",
        "        print(\"Accuracy:\", accuracy)\n",
        "        print(\"Accuracy w/Scaled Data (ss):\", scaled_accuracy)\n",
        "        print(\"Accuracy w/Scaled Data (mm):\", scaled_mm_accuracy)\n",
        "        if (accuracy > scaled_accuracy) and (accuracy > scaled_mm_accuracy):\n",
        "            print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred))\n",
        "            print(\"                            -----------------------------------               \\n\")\n",
        "        elif (scaled_accuracy > scaled_mm_accuracy):\n",
        "            print(\"\\nClassification Report (ss):\\n\", metrics.classification_report(y_test, y_pred_scaled))\n",
        "            print(\"                            -----------------------------------               \\n\")\n",
        "        else:\n",
        "            print(\"\\nClassification Report (mm):\\n\", metrics.classification_report(y_test, y_pred_mm_scaled))\n",
        "            print(\"                            -----------------------------------               \\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "BQJpbtmOlsNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run Decision Trees, Random Forest, KNN and SVM\n",
        "run_models(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pea0lbeylsNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "Searching for better performance out of the models with Gridsearch"
      ],
      "metadata": {
        "id": "j-dycrxVlsNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#model name, classifier, parameters\n",
        "# function used to process models and parameters through gridsearch\n",
        "def hyper_tune(name, clf, parameters, target_names=None):\n",
        "\n",
        "    target_names = target_names\n",
        "    clf = clf\n",
        "    search = GridSearchCV(clf, parameters,verbose=True, n_jobs=15, cv=5)\n",
        "    search.fit(X_train_scaled,y_train)\n",
        "    y_pred_scaled = search.predict(X_test_scaled)\n",
        "    print (\"Accuracy Score = %3.2f\" %(search.score(X_test_scaled,y_test)))\n",
        "    print (search.best_params_)\n",
        "    print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred_scaled, target_names=target_names))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "hHr3ODwzlsNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the KNN model performs better on the unscaled data this function\n",
        "# function for unscaled data\n",
        "#model name, classifier, parameters\n",
        "# function used to process models and parameters through gridsearch\n",
        "def hyper_tune2(name, clf, parameters, target_names=None):\n",
        "\n",
        "    target_names = target_names\n",
        "    clf = clf\n",
        "    search = GridSearchCV(clf, parameters,verbose=True, n_jobs=15, cv=5)\n",
        "    search.fit(X_train,y_train)\n",
        "    y_pred = search.predict(X_test)\n",
        "    print (\"Accuracy Score = %3.2f\" %(search.score(X_test,y_test)))\n",
        "    print (search.best_params_)\n",
        "    print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Mp0JZV4plsNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "plVWhDzdlsNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of neighbors\n",
        "n_neighbors = [int(x) for x in range(4, 15)]\n",
        "# weights\n",
        "weights = ['uniform','distance']\n",
        "# distance metric\n",
        "metric = ['euclidean', 'manhattan', 'chebyshev']\n",
        "# computation algorithm\n",
        "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "# power paramter\n",
        "p=[1,2]\n",
        "\n",
        "parameters = { 'n_neighbors': n_neighbors,\n",
        "              'weights':weights,\n",
        "              'metric':metric,\n",
        "              'p':p,\n",
        "              'algorithm': algorithm\n",
        "               }\n",
        "\n",
        "hyper_tune2('KNN', KNeighborsClassifier(), parameters)"
      ],
      "metadata": {
        "trusted": true,
        "id": "O1s7reTtlsNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "2Dh8J47mlsNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in range(10, 200,10)]\n",
        "# Criterion\n",
        "criterion = ['gini','entropy']\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt', 'log2']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in range(10, 100, 10)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [int(x) for x in range(2, 5)]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [int(x) for x in range(2, 5)]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# random state\n",
        "random_state = [1010]\n",
        "\n",
        "parameters = { 'criterion':criterion,\n",
        "               'n_estimators': n_estimators,\n",
        "              'max_depth':max_depth,\n",
        "              #'random_state': random_state,\n",
        "              #'max_features':max_features,\n",
        "              #'min_samples_split':min_samples_split\n",
        "               }\n",
        "\n",
        "\n",
        "hyper_tune('Random Forest',\n",
        "           RandomForestClassifier(), parameters)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gwzPrWdZlsNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance w/ Random Forest"
      ],
      "metadata": {
        "id": "BtzatopRlsNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Decision Tree classifer object with optimized parameters\n",
        "clf = RandomForestClassifier(criterion='entropy',\n",
        "               n_estimators=52,\n",
        "              max_depth = 51,\n",
        "              max_features='auto',\n",
        "              min_samples_split=2,\n",
        "              random_state=1010)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train_scaled,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8JIqI23flsNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xqXcqq8QlsNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Creating a bar plot\n",
        "sns.barplot(x=feature_imp.index, y=feature_imp)\n",
        "\n",
        "# Add labels to your graph\n",
        "plt.xticks(rotation=45, horizontalalignment='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# create features list\n",
        "features_list = X.columns\n",
        "features_list = features_list.tolist()\n",
        "\n",
        "# Get numerical feature importances\n",
        "importances = list(clf.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]\n",
        "\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances\n",
        "print(\"\\nTop 10 Features:\")\n",
        "display_top = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]\n",
        "\n",
        "# Sort the feature importances by least important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = False)\n",
        "# Print out the feature and importances\n",
        "print(\"\\nBottom 10 Features:\")\n",
        "display_bottom = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "cjWBj2VLlsNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Two Category Targert Variable\n",
        "\n",
        "### Data Prep\n",
        "\n",
        "![](http://)Underweight mapped to being not obese. Arguably however it is just as much of a health concern as being overweight."
      ],
      "metadata": {
        "id": "CQxmHB_xlsNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map values\n",
        "weight_map = { 'Normal_Weight':0, 'Overweight_Level_I':0,\n",
        "               'Overweight_Level_II':0, 'Obesity_Type_I':1,\n",
        "               'Obesity_Type_II':1, 'Obesity_Type_III':1, 'Insufficient_Weight':0}\n",
        "\n",
        "# map values\n",
        "df_prep['weight_cat'] = df_prep['NObeyesdad'].map(weight_map)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Kw945PA9lsNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=\"weight_cat\",\n",
        "                 palette=\"Blues_d\",\n",
        "                 order=df_prep[\"weight_cat\"].value_counts().index,\n",
        "                 data=df_prep)\n",
        "\n",
        "\n",
        "# show figure & plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qlydNJZFlsNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset in features and target variable\n",
        "\n",
        "# Features\n",
        "X = df_prep.drop(columns=[\"NObeyesdad\",\"weight_cat\"])\n",
        "\n",
        "# Target variable\n",
        "y = df_prep['weight_cat']"
      ],
      "metadata": {
        "trusted": true,
        "id": "73mEsvojlsNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "# Scaled version of X train and X test\n",
        "ss = StandardScaler()\n",
        "X_train_scaled = ss.fit_transform(X_train)\n",
        "X_test_scaled = ss.transform(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xKC6EhAdlsNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Random Forest with Two Category Target Variable"
      ],
      "metadata": {
        "id": "VLT2YM1SlsNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in range(10, 200,10)]\n",
        "# Criterion\n",
        "criterion = ['gini','entropy']\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt', 'log2']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in range(10, 100, 10)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [int(x) for x in range(2, 20,2)]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [int(x) for x in range(2, 20, 2)]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# random state\n",
        "random_state = [1010]\n",
        "\n",
        "target_names = ['Not Obese', 'Obese']\n",
        "\n",
        "parameters = { 'criterion':criterion,\n",
        "               'n_estimators': n_estimators,\n",
        "              'max_depth':max_depth,\n",
        "              'random_state': random_state,\n",
        "              'max_features':max_features\n",
        "              #'min_samples_split':min_samples_split\n",
        "               }\n",
        "\n",
        "hyper_tune('Random Forest', RandomForestClassifier(), parameters, target_names=target_names)"
      ],
      "metadata": {
        "trusted": true,
        "id": "oa3_xZTVlsNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance with Random Forest: Two Category Variable Target"
      ],
      "metadata": {
        "id": "4sN0rYqglsNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Random Forest classifer object with optimized parameters\n",
        "clf = RandomForestClassifier(criterion='gini',\n",
        "               n_estimators=110,\n",
        "              max_depth = 20,\n",
        "              max_features='auto',\n",
        "              random_state=1010)\n",
        "\n",
        "# Train Random Forest classifer\n",
        "clf = clf.fit(X_train_scaled,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kkXqseHClsNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "fig = plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Creating a bar plot\n",
        "sns.barplot(x=feature_imp.index, y=feature_imp)\n",
        "\n",
        "# Add labels to your graph\n",
        "plt.xticks(rotation=45, horizontalalignment='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# create features list\n",
        "features_list = X.columns\n",
        "features_list = features_list.tolist()\n",
        "\n",
        "# Get numerical feature importances\n",
        "importances = list(clf.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]\n",
        "\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances\n",
        "print(\"\\nTop 10 Features:\")\n",
        "display_top = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]\n",
        "\n",
        "# Sort the feature importances by least important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = False)\n",
        "# Print out the feature and importances\n",
        "print(\"\\nBottom 10 Features:\")\n",
        "display_bottom = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "da-KqZ-MlsNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}